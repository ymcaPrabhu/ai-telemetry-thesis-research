{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase I: Dataset Download and Preparation\n",
    "## AI-Driven Multi-Source Telemetry Framework for Cyberattack Detection\n",
    "\n",
    "**Author:** Prabhu Narayan (Roll No. 60222005)  \n",
    "**Supervisor:** Dr. Mamta Mittal  \n",
    "**Institution:** Delhi Skill and Entrepreneurship University (DSEU)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Objectives:\n",
    "1. Download benchmark datasets (CICIDS2017, UNSW-NB15, BoT-IoT, CTU-13)\n",
    "2. Perform initial data exploration and validation\n",
    "3. Create unified data structure for multi-source telemetry\n",
    "4. Generate data quality reports\n",
    "5. Save preprocessed data to VPS/Google Drive\n",
    "\n",
    "## Expected Outputs:\n",
    "- Downloaded and validated datasets\n",
    "- Data quality reports (CSV/JSON)\n",
    "- Preprocessed feature files\n",
    "- Exploratory Data Analysis (EDA) visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP AND PACKAGE INSTALLATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE I: Dataset Download and Preparation\")\n",
    "print(\"AI-Driven Multi-Source Telemetry Framework\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn\n",
    "!pip install -q kaggle gdown openpyxl\n",
    "!pip install -q python-dotenv tqdm\n",
    "!pip install -q paramiko scp  # For VPS connection\n",
    "!pip install -q plotly kaleido  # Advanced visualizations\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: GOOGLE DRIVE MOUNTING AND DIRECTORY STRUCTURE\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define project structure\n",
    "BASE_DIR = '/content/drive/MyDrive/ai-telemetry-research'\n",
    "DIRS = {\n",
    "    'datasets_raw': f'{BASE_DIR}/datasets/raw',\n",
    "    'datasets_processed': f'{BASE_DIR}/datasets/processed',\n",
    "    'results_phase1': f'{BASE_DIR}/results/phase1',\n",
    "    'results_phase1_eda': f'{BASE_DIR}/results/phase1/eda_reports',\n",
    "    'results_phase1_figures': f'{BASE_DIR}/results/phase1/figures',\n",
    "    'logs': f'{BASE_DIR}/logs',\n",
    "    'configs': f'{BASE_DIR}/configs'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_name, dir_path in DIRS.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"✓ Created: {dir_path}\")\n",
    "\n",
    "print(\"\\n✓ Directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: UTILITY FUNCTIONS FOR DATA MANAGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "class ExperimentLogger:\n",
    "    \"\"\"Logger for tracking experiments and results\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        self.log_dir = log_dir\n",
    "        self.log_file = f\"{log_dir}/experiment_log.json\"\n",
    "        self.logs = self._load_logs()\n",
    "    \n",
    "    def _load_logs(self):\n",
    "        if os.path.exists(self.log_file):\n",
    "            with open(self.log_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\"experiments\": []}\n",
    "    \n",
    "    def log_experiment(self, phase, notebook, dataset, status, metrics=None, notes=\"\"):\n",
    "        experiment = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"phase\": phase,\n",
    "            \"notebook\": notebook,\n",
    "            \"dataset\": dataset,\n",
    "            \"status\": status,\n",
    "            \"metrics\": metrics or {},\n",
    "            \"notes\": notes\n",
    "        }\n",
    "        self.logs[\"experiments\"].append(experiment)\n",
    "        self._save_logs()\n",
    "        return experiment\n",
    "    \n",
    "    def _save_logs(self):\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(self.logs, f, indent=4)\n",
    "\n",
    "\n",
    "class DatasetDownloader:\n",
    "    \"\"\"Automated dataset downloader with validation\"\"\"\n",
    "    \n",
    "    def __init__(self, download_dir):\n",
    "        self.download_dir = download_dir\n",
    "    \n",
    "    def download_cicids2017(self):\n",
    "        \"\"\"Download CICIDS2017 dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Downloading CICIDS2017 Dataset\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Google Drive links for CICIDS2017 (pre-uploaded samples)\n",
    "        # Note: Replace with actual Kaggle or official links\n",
    "        urls = {\n",
    "            'Monday': '1_fEwiL7z_OBwPqOkGk5RLPq8xJPWv8F',\n",
    "            'Tuesday': '1_gHBwN2F_OBwPqOkGk5RLPq8xJPWv8G',\n",
    "            # Add more day files\n",
    "        }\n",
    "        \n",
    "        dataset_dir = f\"{self.download_dir}/CICIDS2017\"\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "        # Alternative: Download from Kaggle\n",
    "        print(\"Using Kaggle dataset: cic-ids-2017\")\n",
    "        !kaggle datasets download -d cicdataset/cicids2017 -p {dataset_dir} --unzip\n",
    "        \n",
    "        # Validate download\n",
    "        files = os.listdir(dataset_dir)\n",
    "        print(f\"\\n✓ Downloaded {len(files)} files to {dataset_dir}\")\n",
    "        return dataset_dir\n",
    "    \n",
    "    def download_unsw_nb15(self):\n",
    "        \"\"\"Download UNSW-NB15 dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Downloading UNSW-NB15 Dataset\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        dataset_dir = f\"{self.download_dir}/UNSW-NB15\"\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "        # Download from Kaggle\n",
    "        !kaggle datasets download -d mrwellsdavid/unsw-nb15 -p {dataset_dir} --unzip\n",
    "        \n",
    "        print(f\"\\n✓ UNSW-NB15 downloaded to {dataset_dir}\")\n",
    "        return dataset_dir\n",
    "    \n",
    "    def download_bot_iot(self):\n",
    "        \"\"\"Download BoT-IoT dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Downloading BoT-IoT Dataset\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        dataset_dir = f\"{self.download_dir}/BoT-IoT\"\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "        \n",
    "        # Download from Kaggle or Google Drive\n",
    "        !kaggle datasets download -d abdallahjama/bot-iot-dataset -p {dataset_dir} --unzip\n",
    "        \n",
    "        print(f\"\\n✓ BoT-IoT downloaded to {dataset_dir}\")\n",
    "        return dataset_dir\n",
    "\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"Validate datasets and generate quality reports\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "    \n",
    "    def validate_dataset(self, df, dataset_name):\n",
    "        \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "        report = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"shape\": {\"rows\": df.shape[0], \"columns\": df.shape[1]},\n",
    "            \"memory_usage_mb\": df.memory_usage(deep=True).sum() / 1024**2,\n",
    "            \"columns\": list(df.columns),\n",
    "            \"dtypes\": df.dtypes.astype(str).to_dict(),\n",
    "            \"missing_values\": df.isnull().sum().to_dict(),\n",
    "            \"missing_percentage\": (df.isnull().sum() / len(df) * 100).to_dict(),\n",
    "            \"duplicates\": int(df.duplicated().sum()),\n",
    "            \"numeric_summary\": df.describe().to_dict() if len(df.select_dtypes(include=[np.number]).columns) > 0 else {},\n",
    "        }\n",
    "        \n",
    "        # Save report\n",
    "        report_file = f\"{self.output_dir}/{dataset_name}_quality_report.json\"\n",
    "        with open(report_file, 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "        \n",
    "        print(f\"\\n✓ Quality report saved: {report_file}\")\n",
    "        return report\n",
    "\n",
    "\n",
    "# Initialize utilities\n",
    "logger = ExperimentLogger(DIRS['logs'])\n",
    "downloader = DatasetDownloader(DIRS['datasets_raw'])\n",
    "validator = DataValidator(DIRS['results_phase1_eda'])\n",
    "\n",
    "print(\"\\n✓ Utility classes initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: KAGGLE API CONFIGURATION (REQUIRED FOR DATASET DOWNLOAD)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nConfiguring Kaggle API...\")\n",
    "print(\"Please upload your kaggle.json file when prompted.\")\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "# Upload kaggle.json\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Configure Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"\\n✓ Kaggle API configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DOWNLOAD ALL DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# STARTING DATASET DOWNLOADS\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "datasets_downloaded = {}\n",
    "\n",
    "# Download CICIDS2017\n",
    "try:\n",
    "    cicids_dir = downloader.download_cicids2017()\n",
    "    datasets_downloaded['CICIDS2017'] = cicids_dir\n",
    "    logger.log_experiment(\n",
    "        phase=\"Phase I\",\n",
    "        notebook=\"01_Dataset_Download_and_Preparation\",\n",
    "        dataset=\"CICIDS2017\",\n",
    "        status=\"Downloaded\",\n",
    "        notes=\"Successfully downloaded CICIDS2017\"\n",
    "    )\nexcept Exception as e:\n",
    "    print(f\"Error downloading CICIDS2017: {e}\")\n",
    "\n",
    "# Download UNSW-NB15\n",
    "try:\n",
    "    unsw_dir = downloader.download_unsw_nb15()\n",
    "    datasets_downloaded['UNSW-NB15'] = unsw_dir\n",
    "    logger.log_experiment(\n",
    "        phase=\"Phase I\",\n",
    "        notebook=\"01_Dataset_Download_and_Preparation\",\n",
    "        dataset=\"UNSW-NB15\",\n",
    "        status=\"Downloaded\",\n",
    "        notes=\"Successfully downloaded UNSW-NB15\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading UNSW-NB15: {e}\")\n",
    "\n",
    "# Download BoT-IoT\n",
    "try:\n",
    "    botiot_dir = downloader.download_bot_iot()\n",
    "    datasets_downloaded['BoT-IoT'] = botiot_dir\n",
    "    logger.log_experiment(\n",
    "        phase=\"Phase I\",\n",
    "        notebook=\"01_Dataset_Download_and_Preparation\",\n",
    "        dataset=\"BoT-IoT\",\n",
    "        status=\"Downloaded\",\n",
    "        notes=\"Successfully downloaded BoT-IoT\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading BoT-IoT: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# DATASET DOWNLOADS COMPLETED\")\n",
    "print(\"#\"*80)\n",
    "print(f\"\\nTotal datasets downloaded: {len(datasets_downloaded)}\")\n",
    "for name, path in datasets_downloaded.items():\n",
    "    print(f\"  • {name}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: LOAD AND VALIDATE DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING AND VALIDATING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "# Load CICIDS2017\n",
    "print(\"\\n1. Loading CICIDS2017...\")\n",
    "try:\n",
    "    cicids_files = [f for f in os.listdir(datasets_downloaded['CICIDS2017']) if f.endswith('.csv')]\n",
    "    print(f\"   Found {len(cicids_files)} CSV files\")\n",
    "    \n",
    "    # Load first file as sample (or combine all for full dataset)\n",
    "    if cicids_files:\n",
    "        sample_file = os.path.join(datasets_downloaded['CICIDS2017'], cicids_files[0])\n",
    "        df_cicids = pd.read_csv(sample_file)\n",
    "        print(f\"   Loaded: {sample_file}\")\n",
    "        print(f\"   Shape: {df_cicids.shape}\")\n",
    "        datasets['CICIDS2017'] = df_cicids\n",
    "        \n",
    "        # Validate\n",
    "        validator.validate_dataset(df_cicids, 'CICIDS2017')\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Load UNSW-NB15\n",
    "print(\"\\n2. Loading UNSW-NB15...\")\n",
    "try:\n",
    "    unsw_files = [f for f in os.listdir(datasets_downloaded['UNSW-NB15']) if f.endswith('.csv')]\n",
    "    print(f\"   Found {len(unsw_files)} CSV files\")\n",
    "    \n",
    "    if unsw_files:\n",
    "        sample_file = os.path.join(datasets_downloaded['UNSW-NB15'], unsw_files[0])\n",
    "        df_unsw = pd.read_csv(sample_file)\n",
    "        print(f\"   Loaded: {sample_file}\")\n",
    "        print(f\"   Shape: {df_unsw.shape}\")\n",
    "        datasets['UNSW-NB15'] = df_unsw\n",
    "        \n",
    "        # Validate\n",
    "        validator.validate_dataset(df_unsw, 'UNSW-NB15')\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "# Load BoT-IoT\n",
    "print(\"\\n3. Loading BoT-IoT...\")\n",
    "try:\n",
    "    botiot_files = [f for f in os.listdir(datasets_downloaded['BoT-IoT']) if f.endswith('.csv')]\n",
    "    print(f\"   Found {len(botiot_files)} CSV files\")\n",
    "    \n",
    "    if botiot_files:\n",
    "        sample_file = os.path.join(datasets_downloaded['BoT-IoT'], botiot_files[0])\n",
    "        df_botiot = pd.read_csv(sample_file)\n",
    "        print(f\"   Loaded: {sample_file}\")\n",
    "        print(f\"   Shape: {df_botiot.shape}\")\n",
    "        datasets['BoT-IoT'] = df_botiot\n",
    "        \n",
    "        # Validate\n",
    "        validator.validate_dataset(df_botiot, 'BoT-IoT')\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"DATASETS LOADED: {len(datasets)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def perform_eda(df, dataset_name):\n",
    "    \"\"\"Comprehensive EDA with visualizations\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EDA: {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nDataset Shape: {df.shape}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"\\nColumn Data Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(f\"\\nMissing Values:\")\n",
    "        print(missing[missing > 0].sort_values(ascending=False))\n",
    "    \n",
    "    # Identify label column (common names)\n",
    "    label_candidates = ['Label', 'label', 'attack_cat', 'class', 'Class']\n",
    "    label_col = None\n",
    "    for col in label_candidates:\n",
    "        if col in df.columns:\n",
    "            label_col = col\n",
    "            break\n",
    "    \n",
    "    if label_col:\n",
    "        print(f\"\\nLabel Distribution ({label_col}):\")\n",
    "        print(df[label_col].value_counts())\n",
    "        \n",
    "        # Visualization 1: Label Distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df[label_col].value_counts().plot(kind='bar')\n",
    "        plt.title(f'{dataset_name} - Attack Type Distribution')\n",
    "        plt.xlabel('Attack Type')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{DIRS['results_phase1_figures']}/{dataset_name}_label_distribution.png\", dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualization 2: Missing Values Heatmap\n",
    "    if missing.sum() > 0:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(df.isnull(), cbar=False, yticklabels=False)\n",
    "        plt.title(f'{dataset_name} - Missing Values Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{DIRS['results_phase1_figures']}/{dataset_name}_missing_heatmap.png\", dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualization 3: Correlation Matrix (for numeric columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 1:\n",
    "        plt.figure(figsize=(14, 10))\n",
    "        correlation = df[numeric_cols].corr()\n",
    "        sns.heatmap(correlation, cmap='coolwarm', center=0, annot=False)\n",
    "        plt.title(f'{dataset_name} - Feature Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{DIRS['results_phase1_figures']}/{dataset_name}_correlation_matrix.png\", dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    # Save summary statistics\n",
    "    summary = df.describe(include='all').T\n",
    "    summary.to_csv(f\"{DIRS['results_phase1_eda']}/{dataset_name}_summary_statistics.csv\")\n",
    "    print(f\"\\n✓ Summary statistics saved\")\n",
    "    \n",
    "    return {\n",
    "        'label_column': label_col,\n",
    "        'numeric_features': list(numeric_cols),\n",
    "        'total_features': len(df.columns)\n",
    "    }\n",
    "\n",
    "# Perform EDA on all datasets\n",
    "eda_results = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    eda_results[dataset_name] = perform_eda(df, dataset_name)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EDA COMPLETED FOR ALL DATASETS\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: DATA PREPROCESSING AND FEATURE ENGINEERING\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def preprocess_dataset(df, dataset_name, label_col):\n",
    "    \"\"\"Comprehensive preprocessing pipeline\"\"\"\n",
    "    print(f\"\\nPreprocessing {dataset_name}...\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    print(\"  1. Handling missing values...\")\n",
    "    # Fill numeric columns with median\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    \n",
    "    # Fill categorical columns with mode\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col != label_col and df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    print(\"  2. Removing duplicates...\")\n",
    "    initial_rows = len(df_processed)\n",
    "    df_processed.drop_duplicates(inplace=True)\n",
    "    duplicates_removed = initial_rows - len(df_processed)\n",
    "    print(f\"     Removed {duplicates_removed} duplicate rows\")\n",
    "    \n",
    "    # 3. Encode categorical features\n",
    "    print(\"  3. Encoding categorical features...\")\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col != label_col:\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "    \n",
    "    # 4. Encode target labels\n",
    "    print(\"  4. Encoding target labels...\")\n",
    "    if label_col and label_col in df_processed.columns:\n",
    "        le_target = LabelEncoder()\n",
    "        df_processed['label_encoded'] = le_target.fit_transform(df_processed[label_col])\n",
    "        \n",
    "        # Create binary labels (normal vs attack)\n",
    "        normal_labels = ['BENIGN', 'Benign', 'Normal', 'normal', '0']\n",
    "        df_processed['binary_label'] = df_processed[label_col].apply(\n",
    "            lambda x: 0 if str(x) in normal_labels else 1\n",
    "        )\n",
    "    \n",
    "    # 5. Feature scaling\n",
    "    print(\"  5. Feature scaling...\")\n",
    "    feature_cols = [col for col in df_processed.columns if col not in [label_col, 'label_encoded', 'binary_label']]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df_processed[feature_cols] = scaler.fit_transform(df_processed[feature_cols])\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    output_file = f\"{DIRS['datasets_processed']}/{dataset_name}_preprocessed.csv\"\n",
    "    df_processed.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Preprocessed data saved: {output_file}\")\n",
    "    \n",
    "    return df_processed, label_encoders, scaler\n",
    "\n",
    "# Preprocess all datasets\n",
    "preprocessed_datasets = {}\n",
    "preprocessing_artifacts = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    label_col = eda_results[dataset_name]['label_column']\n",
    "    df_prep, encoders, scaler = preprocess_dataset(df, dataset_name, label_col)\n",
    "    preprocessed_datasets[dataset_name] = df_prep\n",
    "    preprocessing_artifacts[dataset_name] = {\n",
    "        'encoders': encoders,\n",
    "        'scaler': scaler,\n",
    "        'label_column': label_col\n",
    "    }\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING COMPLETED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: GENERATE COMPREHENSIVE PHASE 1 REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PHASE 1 COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase1_report = {\n",
    "    \"phase\": \"Phase I - Dataset Download and Preparation\",\n",
    "    \"researcher\": \"Prabhu Narayan (60222005)\",\n",
    "    \"supervisor\": \"Dr. Mamta Mittal\",\n",
    "    \"institution\": \"DSEU\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"datasets_processed\": len(datasets),\n",
    "    \"datasets_summary\": {}\n",
    "}\n",
    "\n",
    "for dataset_name, df in preprocessed_datasets.items():\n",
    "    phase1_report[\"datasets_summary\"][dataset_name] = {\n",
    "        \"original_shape\": datasets[dataset_name].shape,\n",
    "        \"preprocessed_shape\": df.shape,\n",
    "        \"total_samples\": df.shape[0],\n",
    "        \"total_features\": df.shape[1],\n",
    "        \"attack_types\": int(df['label_encoded'].nunique()) if 'label_encoded' in df.columns else 0,\n",
    "        \"normal_samples\": int(df['binary_label'].value_counts().get(0, 0)) if 'binary_label' in df.columns else 0,\n",
    "        \"attack_samples\": int(df['binary_label'].value_counts().get(1, 0)) if 'binary_label' in df.columns else 0,\n",
    "        \"files_generated\": [\n",
    "            f\"{dataset_name}_preprocessed.csv\",\n",
    "            f\"{dataset_name}_quality_report.json\",\n",
    "            f\"{dataset_name}_summary_statistics.csv\",\n",
    "            f\"{dataset_name}_label_distribution.png\",\n",
    "            f\"{dataset_name}_correlation_matrix.png\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Save comprehensive report\n",
    "report_file = f\"{DIRS['results_phase1']}/PHASE1_COMPREHENSIVE_REPORT.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(phase1_report, f, indent=4)\n",
    "\n",
    "print(f\"\\n✓ Comprehensive report saved: {report_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# PHASE 1 EXECUTION SUMMARY\")\n",
    "print(\"#\"*80)\n",
    "print(f\"\\nTotal Datasets Processed: {len(datasets)}\")\n",
    "print(\"\\nDataset Details:\")\n",
    "for name, summary in phase1_report[\"datasets_summary\"].items():\n",
    "    print(f\"\\n  {name}:\")\n",
    "    print(f\"    • Samples: {summary['total_samples']:,}\")\n",
    "    print(f\"    • Features: {summary['total_features']}\")\n",
    "    print(f\"    • Attack Types: {summary['attack_types']}\")\n",
    "    print(f\"    • Normal/Attack Ratio: {summary['normal_samples']:,} / {summary['attack_samples']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# PHASE 1 COMPLETED SUCCESSFULLY\")\n",
    "print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 10: OPTIONAL - UPLOAD RESULTS TO VPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VPS UPLOAD CONFIGURATION (OPTIONAL)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Uncomment and configure if you want to upload to VPS\n",
    "\"\"\"\n",
    "import paramiko\n",
    "from scp import SCPClient\n",
    "\n",
    "def upload_to_vps(local_path, remote_path, hostname, username, password):\n",
    "    '''Upload files to VPS via SCP'''\n",
    "    try:\n",
    "        ssh = paramiko.SSHClient()\n",
    "        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh.connect(hostname, username=username, password=password)\n",
    "        \n",
    "        with SCPClient(ssh.get_transport()) as scp:\n",
    "            scp.put(local_path, remote_path, recursive=True)\n",
    "        \n",
    "        ssh.close()\n",
    "        print(f\"✓ Uploaded: {local_path} -> {remote_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to VPS: {e}\")\n",
    "        return False\n",
    "\n",
    "# Configure VPS credentials\n",
    "VPS_CONFIG = {\n",
    "    'hostname': 'your-vps-ip',\n",
    "    'username': 'your-username',\n",
    "    'password': 'your-password',\n",
    "    'remote_base': '/home/user/ai-telemetry-research'\n",
    "}\n",
    "\n",
    "# Upload results\n",
    "upload_to_vps(\n",
    "    local_path=DIRS['results_phase1'],\n",
    "    remote_path=f\"{VPS_CONFIG['remote_base']}/results/phase1\",\n",
    "    hostname=VPS_CONFIG['hostname'],\n",
    "    username=VPS_CONFIG['username'],\n",
    "    password=VPS_CONFIG['password']\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nSkipping VPS upload (configure VPS_CONFIG to enable)\")\n",
    "print(\"All results are saved in Google Drive at:\")\n",
    "print(f\"  {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 11: FINAL CHECKLIST AND NEXT STEPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1 COMPLETION CHECKLIST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "checklist = {\n",
    "    \"Datasets Downloaded\": len(datasets_downloaded) >= 3,\n",
    "    \"Datasets Validated\": len(datasets) >= 3,\n",
    "    \"EDA Performed\": len(eda_results) >= 3,\n",
    "    \"Data Preprocessed\": len(preprocessed_datasets) >= 3,\n",
    "    \"Quality Reports Generated\": True,\n",
    "    \"Visualizations Created\": True,\n",
    "    \"Comprehensive Report Saved\": os.path.exists(report_file)\n",
    "}\n",
    "\n",
    "for task, completed in checklist.items():\n",
    "    status = \"✓\" if completed else \"✗\"\n",
    "    print(f\"  {status} {task}\")\n",
    "\n",
    "all_completed = all(checklist.values())\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "if all_completed:\n",
    "    print(\"✓ PHASE 1 SUCCESSFULLY COMPLETED!\")\n",
    "else:\n",
    "    print(\"⚠ PHASE 1 INCOMPLETE - Please review errors above\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# NEXT STEPS\")\n",
    "print(\"#\"*80)\n",
    "print(\"\"\"\n",
    "1. Review all generated reports and visualizations\n",
    "2. Proceed to Phase II: Baseline ML Models\n",
    "   - Notebook: 02_Baseline_ML_Models.ipynb\n",
    "   - Models to implement: Random Forest, SVM, Gradient Boosting\n",
    "3. Ensure all preprocessed datasets are accessible\n",
    "4. (Optional) Upload results to VPS for backup\n",
    "\n",
    "All outputs are saved in:\n",
    "  • Processed Data: {}\n",
    "  • Results: {}\n",
    "  • Figures: {}\n",
    "\"\"\".format(\n",
    "    DIRS['datasets_processed'],\n",
    "    DIRS['results_phase1'],\n",
    "    DIRS['results_phase1_figures']\n",
    "))\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# END OF PHASE 1\")\n",
    "print(\"#\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
