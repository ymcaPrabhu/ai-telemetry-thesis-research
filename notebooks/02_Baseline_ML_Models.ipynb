{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II: Baseline Machine Learning Models\n",
    "## AI-Driven Multi-Source Telemetry Framework for Cyberattack Detection\n",
    "\n",
    "**Author:** Prabhu Narayan (Roll No. 60222005)  \n",
    "**Supervisor:** Dr. Mamta Mittal  \n",
    "**Institution:** Delhi Skill and Entrepreneurship University (DSEU)\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Objectives:\n",
    "1. Train baseline ML models (Random Forest, SVM, XGBoost, Gradient Boosting)\n",
    "2. Perform comprehensive model evaluation (Accuracy, Precision, Recall, F1-Score, AUC-ROC)\n",
    "3. Compare model performance across datasets\n",
    "4. Generate confusion matrices and classification reports\n",
    "5. Save trained models and performance metrics\n",
    "\n",
    "## Models to Implement:\n",
    "- **Random Forest Classifier**\n",
    "- **Support Vector Machine (SVM)**\n",
    "- **XGBoost Classifier**\n",
    "- **Gradient Boosting Classifier**\n",
    "- **Ensemble Model (Voting Classifier)**\n",
    "\n",
    "## Expected Outputs:\n",
    "- Trained model files (.pkl)\n",
    "- Performance metrics (JSON/CSV)\n",
    "- Confusion matrices and ROC curves\n",
    "- Comparative analysis visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 1: ENVIRONMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE II: Baseline Machine Learning Models\")\n",
    "print(\"AI-Driven Multi-Source Telemetry Framework\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q pandas numpy scikit-learn matplotlib seaborn\n",
    "!pip install -q xgboost lightgbm catboost\n",
    "!pip install -q imbalanced-learn\n",
    "!pip install -q joblib pickle5\n",
    "\n",
    "print(\"\\n✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import joblib\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# XGBoost and LightGBM\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: GOOGLE DRIVE MOUNTING AND DIRECTORY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define project structure\n",
    "BASE_DIR = '/content/drive/MyDrive/ai-telemetry-research'\n",
    "DIRS = {\n",
    "    'datasets_processed': f'{BASE_DIR}/datasets/processed',\n",
    "    'models_baseline': f'{BASE_DIR}/models/baseline_ml',\n",
    "    'results_phase2': f'{BASE_DIR}/results/phase2',\n",
    "    'results_phase2_metrics': f'{BASE_DIR}/results/phase2/metrics',\n",
    "    'results_phase2_figures': f'{BASE_DIR}/results/phase2/figures',\n",
    "    'results_phase2_models': f'{BASE_DIR}/results/phase2/trained_models',\n",
    "    'logs': f'{BASE_DIR}/logs'\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "for dir_name, dir_path in DIRS.items():\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"✓ Directory structure created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: UTILITY CLASSES FOR MODEL TRAINING AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Comprehensive model training and evaluation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "    \n",
    "    def prepare_data(self, df, label_col='binary_label', test_size=0.3):\n",
    "        \"\"\"Prepare train-test split\"\"\"\n",
    "        print(f\"\\nPreparing data with label column: {label_col}\")\n",
    "        \n",
    "        # Separate features and labels\n",
    "        X = df.drop([label_col], axis=1, errors='ignore')\n",
    "        \n",
    "        # Remove non-numeric columns\n",
    "        X = X.select_dtypes(include=[np.number])\n",
    "        y = df[label_col]\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        print(f\"  Training set: {X_train.shape}\")\n",
    "        print(f\"  Test set: {X_test.shape}\")\n",
    "        print(f\"  Features: {X_train.shape[1]}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def handle_imbalance(self, X_train, y_train, method='smote'):\n",
    "        \"\"\"Handle class imbalance using SMOTE or undersampling\"\"\"\n",
    "        print(f\"\\nOriginal class distribution: {dict(pd.Series(y_train).value_counts())}\")\n",
    "        \n",
    "        if method == 'smote':\n",
    "            smote = SMOTE(random_state=self.random_state)\n",
    "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "        elif method == 'undersample':\n",
    "            rus = RandomUnderSampler(random_state=self.random_state)\n",
    "            X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "        else:\n",
    "            return X_train, y_train\n",
    "        \n",
    "        print(f\"Resampled class distribution: {dict(pd.Series(y_resampled).value_counts())}\")\n",
    "        return X_resampled, y_resampled\n",
    "    \n",
    "    def train_random_forest(self, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train Random Forest Classifier\"\"\"\n",
    "        print(\"\\nTraining Random Forest...\")\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': kwargs.get('n_estimators', 100),\n",
    "            'max_depth': kwargs.get('max_depth', 20),\n",
    "            'min_samples_split': kwargs.get('min_samples_split', 5),\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['random_forest'] = model\n",
    "        print(\"✓ Random Forest trained successfully\")\n",
    "        return model\n",
    "    \n",
    "    def train_svm(self, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train Support Vector Machine\"\"\"\n",
    "        print(\"\\nTraining SVM...\")\n",
    "        \n",
    "        params = {\n",
    "            'C': kwargs.get('C', 1.0),\n",
    "            'kernel': kwargs.get('kernel', 'rbf'),\n",
    "            'gamma': kwargs.get('gamma', 'scale'),\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        # Use subset for SVM due to computational cost\n",
    "        if len(X_train) > 10000:\n",
    "            print(\"  Using subset of data for SVM training (10,000 samples)\")\n",
    "            indices = np.random.choice(len(X_train), 10000, replace=False)\n",
    "            X_train_subset = X_train.iloc[indices]\n",
    "            y_train_subset = y_train.iloc[indices]\n",
    "        else:\n",
    "            X_train_subset = X_train\n",
    "            y_train_subset = y_train\n",
    "        \n",
    "        model = SVC(**params, probability=True)\n",
    "        model.fit(X_train_subset, y_train_subset)\n",
    "        \n",
    "        self.models['svm'] = model\n",
    "        print(\"✓ SVM trained successfully\")\n",
    "        return model\n",
    "    \n",
    "    def train_xgboost(self, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train XGBoost Classifier\"\"\"\n",
    "        print(\"\\nTraining XGBoost...\")\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': kwargs.get('n_estimators', 100),\n",
    "            'max_depth': kwargs.get('max_depth', 10),\n",
    "            'learning_rate': kwargs.get('learning_rate', 0.1),\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1,\n",
    "            'eval_metric': 'logloss'\n",
    "        }\n",
    "        \n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['xgboost'] = model\n",
    "        print(\"✓ XGBoost trained successfully\")\n",
    "        return model\n",
    "    \n",
    "    def train_gradient_boosting(self, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train Gradient Boosting Classifier\"\"\"\n",
    "        print(\"\\nTraining Gradient Boosting...\")\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': kwargs.get('n_estimators', 100),\n",
    "            'max_depth': kwargs.get('max_depth', 5),\n",
    "            'learning_rate': kwargs.get('learning_rate', 0.1),\n",
    "            'random_state': self.random_state\n",
    "        }\n",
    "        \n",
    "        model = GradientBoostingClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['gradient_boosting'] = model\n",
    "        print(\"✓ Gradient Boosting trained successfully\")\n",
    "        return model\n",
    "    \n",
    "    def train_lightgbm(self, X_train, y_train, **kwargs):\n",
    "        \"\"\"Train LightGBM Classifier\"\"\"\n",
    "        print(\"\\nTraining LightGBM...\")\n",
    "        \n",
    "        params = {\n",
    "            'n_estimators': kwargs.get('n_estimators', 100),\n",
    "            'max_depth': kwargs.get('max_depth', 10),\n",
    "            'learning_rate': kwargs.get('learning_rate', 0.1),\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1,\n",
    "            'verbose': -1\n",
    "        }\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        self.models['lightgbm'] = model\n",
    "        print(\"✓ LightGBM trained successfully\")\n",
    "        return model\n",
    "    \n",
    "    def create_ensemble(self, X_train, y_train):\n",
    "        \"\"\"Create ensemble voting classifier\"\"\"\n",
    "        print(\"\\nCreating Ensemble Model...\")\n",
    "        \n",
    "        # Train individual models\n",
    "        self.train_random_forest(X_train, y_train)\n",
    "        self.train_xgboost(X_train, y_train)\n",
    "        self.train_lightgbm(X_train, y_train)\n",
    "        \n",
    "        # Create ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('rf', self.models['random_forest']),\n",
    "                ('xgb', self.models['xgboost']),\n",
    "                ('lgb', self.models['lightgbm'])\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        ensemble.fit(X_train, y_train)\n",
    "        self.models['ensemble'] = ensemble\n",
    "        \n",
    "        print(\"✓ Ensemble model created successfully\")\n",
    "        return ensemble\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, model_name):\n",
    "        \"\"\"Comprehensive model evaluation\"\"\"\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "            'f1_score': f1_score(y_test, y_pred, average='binary', zero_division=0),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
    "        }\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Classification report\n",
    "        class_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "        print(f\"  Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"  F1-Score:  {metrics['f1_score']:.4f}\")\n",
    "        if metrics['roc_auc']:\n",
    "            print(f\"  ROC-AUC:   {metrics['roc_auc']:.4f}\")\n",
    "        \n",
    "        self.results[model_name] = {\n",
    "            'metrics': metrics,\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'predictions': {\n",
    "                'y_pred': y_pred.tolist()[:100],  # Save first 100 predictions\n",
    "                'y_true': y_test.tolist()[:100]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return metrics, cm, class_report\n",
    "\n",
    "\n",
    "class ResultVisualizer:\n",
    "    \"\"\"Visualization utilities for model results\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, model_name, dataset_name):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "        plt.title(f'Confusion Matrix - {model_name} on {dataset_name}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"{self.output_dir}/cm_{model_name}_{dataset_name}.png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "        return filename\n",
    "    \n",
    "    def plot_roc_curve(self, models, X_test, y_test, dataset_name):\n",
    "        \"\"\"Plot ROC curves for multiple models\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "                auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "                plt.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves - {dataset_name}')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        filename = f\"{self.output_dir}/roc_curves_{dataset_name}.png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "        return filename\n",
    "    \n",
    "    def plot_model_comparison(self, results_dict, dataset_name):\n",
    "        \"\"\"Compare performance metrics across models\"\"\"\n",
    "        metrics_df = pd.DataFrame([\n",
    "            results['metrics'] for results in results_dict.values()\n",
    "        ])\n",
    "        \n",
    "        # Bar plot comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle(f'Model Performance Comparison - {dataset_name}', fontsize=16)\n",
    "        \n",
    "        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "        \n",
    "        for idx, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[idx // 2, idx % 2]\n",
    "            metrics_df.plot(x='model_name', y=metric, kind='bar', ax=ax, legend=False)\n",
    "            ax.set_title(metric.replace('_', ' ').title())\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel('Score')\n",
    "            ax.set_ylim([0, 1])\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        filename = f\"{self.output_dir}/model_comparison_{dataset_name}.png\"\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.show()\n",
    "        return filename\n",
    "\n",
    "\n",
    "print(\"\\n✓ Utility classes initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: LOAD PREPROCESSED DATASETS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING PREPROCESSED DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {}\n",
    "processed_dir = DIRS['datasets_processed']\n",
    "\n",
    "# Find all preprocessed CSV files\n",
    "preprocessed_files = [f for f in os.listdir(processed_dir) if f.endswith('_preprocessed.csv')]\n",
    "\n",
    "print(f\"\\nFound {len(preprocessed_files)} preprocessed datasets\")\n",
    "\n",
    "for file in preprocessed_files:\n",
    "    dataset_name = file.replace('_preprocessed.csv', '')\n",
    "    filepath = os.path.join(processed_dir, file)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        datasets[dataset_name] = df\n",
    "        print(f\"\\n✓ Loaded {dataset_name}\")\n",
    "        print(f\"  Shape: {df.shape}\")\n",
    "        print(f\"  Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Check for label columns\n",
    "        if 'binary_label' in df.columns:\n",
    "            print(f\"  Binary labels: {dict(df['binary_label'].value_counts())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ Error loading {dataset_name}: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"TOTAL DATASETS LOADED: {len(datasets)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: TRAIN AND EVALUATE BASELINE MODELS ON EACH DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# STARTING MODEL TRAINING AND EVALUATION\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "all_results = {}\n",
    "all_models = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"PROCESSING DATASET: {dataset_name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize trainer and visualizer\n",
    "    trainer = ModelTrainer(random_state=RANDOM_STATE)\n",
    "    visualizer = ResultVisualizer(DIRS['results_phase2_figures'])\n",
    "    \n",
    "    # Prepare data\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = trainer.prepare_data(df, label_col='binary_label')\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing data: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Handle class imbalance\n",
    "    X_train_balanced, y_train_balanced = trainer.handle_imbalance(X_train, y_train, method='smote')\n",
    "    \n",
    "    # Train models\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"TRAINING MODELS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    # 1. Random Forest\n",
    "    try:\n",
    "        rf_model = trainer.train_random_forest(X_train_balanced, y_train_balanced, n_estimators=100)\n",
    "        rf_metrics, rf_cm, rf_report = trainer.evaluate_model(rf_model, X_test, y_test, 'random_forest')\n",
    "        visualizer.plot_confusion_matrix(rf_cm, 'RandomForest', dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Random Forest: {e}\")\n",
    "    \n",
    "    # 2. XGBoost\n",
    "    try:\n",
    "        xgb_model = trainer.train_xgboost(X_train_balanced, y_train_balanced, n_estimators=100)\n",
    "        xgb_metrics, xgb_cm, xgb_report = trainer.evaluate_model(xgb_model, X_test, y_test, 'xgboost')\n",
    "        visualizer.plot_confusion_matrix(xgb_cm, 'XGBoost', dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training XGBoost: {e}\")\n",
    "    \n",
    "    # 3. LightGBM\n",
    "    try:\n",
    "        lgb_model = trainer.train_lightgbm(X_train_balanced, y_train_balanced, n_estimators=100)\n",
    "        lgb_metrics, lgb_cm, lgb_report = trainer.evaluate_model(lgb_model, X_test, y_test, 'lightgbm')\n",
    "        visualizer.plot_confusion_matrix(lgb_cm, 'LightGBM', dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training LightGBM: {e}\")\n",
    "    \n",
    "    # 4. Gradient Boosting\n",
    "    try:\n",
    "        gb_model = trainer.train_gradient_boosting(X_train_balanced, y_train_balanced, n_estimators=50)\n",
    "        gb_metrics, gb_cm, gb_report = trainer.evaluate_model(gb_model, X_test, y_test, 'gradient_boosting')\n",
    "        visualizer.plot_confusion_matrix(gb_cm, 'GradientBoosting', dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Gradient Boosting: {e}\")\n",
    "    \n",
    "    # 5. SVM (on subset due to computational cost)\n",
    "    try:\n",
    "        svm_model = trainer.train_svm(X_train_balanced, y_train_balanced)\n",
    "        svm_metrics, svm_cm, svm_report = trainer.evaluate_model(svm_model, X_test, y_test, 'svm')\n",
    "        visualizer.plot_confusion_matrix(svm_cm, 'SVM', dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error training SVM: {e}\")\n",
    "    \n",
    "    # Plot ROC curves\n",
    "    try:\n",
    "        visualizer.plot_roc_curve(trainer.models, X_test, y_test, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting ROC curves: {e}\")\n",
    "    \n",
    "    # Plot model comparison\n",
    "    try:\n",
    "        visualizer.plot_model_comparison(trainer.results, dataset_name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting model comparison: {e}\")\n",
    "    \n",
    "    # Save results\n",
    "    all_results[dataset_name] = trainer.results\n",
    "    all_models[dataset_name] = trainer.models\n",
    "    \n",
    "    # Save models\n",
    "    for model_name, model in trainer.models.items():\n",
    "        model_file = f\"{DIRS['results_phase2_models']}/{dataset_name}_{model_name}.pkl\"\n",
    "        joblib.dump(model, model_file)\n",
    "        print(f\"\\n✓ Saved model: {model_file}\")\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    metrics_file = f\"{DIRS['results_phase2_metrics']}/{dataset_name}_metrics.json\"\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(trainer.results, f, indent=4)\n",
    "    print(f\"✓ Saved metrics: {metrics_file}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# MODEL TRAINING COMPLETED\")\n",
    "print(\"#\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: CROSS-DATASET PERFORMANCE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DATASET PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Aggregate metrics across all datasets\n",
    "all_metrics = []\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    for model_name, model_results in results.items():\n",
    "        metrics = model_results['metrics'].copy()\n",
    "        metrics['dataset'] = dataset_name\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "# Create comprehensive metrics DataFrame\n",
    "metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "print(\"\\nOverall Performance Summary:\")\n",
    "print(metrics_df.groupby('model_name')[['accuracy', 'precision', 'recall', 'f1_score']].mean())\n",
    "\n",
    "# Save comprehensive metrics\n",
    "metrics_df.to_csv(f\"{DIRS['results_phase2']}/comprehensive_metrics.csv\", index=False)\n",
    "print(f\"\\n✓ Saved comprehensive metrics CSV\")\n",
    "\n",
    "# Visualization: Heatmap of model performance across datasets\n",
    "plt.figure(figsize=(14, 8))\n",
    "pivot_accuracy = metrics_df.pivot(index='model_name', columns='dataset', values='accuracy')\n",
    "sns.heatmap(pivot_accuracy, annot=True, fmt='.3f', cmap='YlGnBu', cbar_kws={'label': 'Accuracy'})\n",
    "plt.title('Model Accuracy Across Datasets')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DIRS['results_phase2_figures']}/accuracy_heatmap_all_datasets.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Box plot of F1-scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='model_name', y='f1_score', data=metrics_df)\n",
    "plt.title('F1-Score Distribution Across Models and Datasets')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{DIRS['results_phase2_figures']}/f1_score_boxplot.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DATASET ANALYSIS COMPLETED\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: GENERATE PHASE 2 COMPREHENSIVE REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING PHASE 2 COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "phase2_report = {\n",
    "    \"phase\": \"Phase II - Baseline Machine Learning Models\",\n",
    "    \"researcher\": \"Prabhu Narayan (60222005)\",\n",
    "    \"supervisor\": \"Dr. Mamta Mittal\",\n",
    "    \"institution\": \"DSEU\",\n",
    "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"datasets_used\": list(datasets.keys()),\n",
    "    \"models_trained\": ['Random Forest', 'XGBoost', 'LightGBM', 'Gradient Boosting', 'SVM'],\n",
    "    \"total_models\": sum([len(models) for models in all_models.values()]),\n",
    "    \"performance_summary\": {}\n",
    "}\n",
    "\n",
    "# Best performing model per dataset\n",
    "for dataset_name, results in all_results.items():\n",
    "    best_model = max(results.items(), key=lambda x: x[1]['metrics']['f1_score'])\n",
    "    phase2_report[\"performance_summary\"][dataset_name] = {\n",
    "        \"best_model\": best_model[0],\n",
    "        \"best_f1_score\": best_model[1]['metrics']['f1_score'],\n",
    "        \"best_accuracy\": best_model[1]['metrics']['accuracy'],\n",
    "        \"all_models\": {k: v['metrics'] for k, v in results.items()}\n",
    "    }\n",
    "\n",
    "# Overall best model\n",
    "avg_performance = metrics_df.groupby('model_name')[['accuracy', 'f1_score']].mean()\n",
    "overall_best = avg_performance['f1_score'].idxmax()\n",
    "phase2_report[\"overall_best_model\"] = {\n",
    "    \"model_name\": overall_best,\n",
    "    \"avg_accuracy\": float(avg_performance.loc[overall_best, 'accuracy']),\n",
    "    \"avg_f1_score\": float(avg_performance.loc[overall_best, 'f1_score'])\n",
    "}\n",
    "\n",
    "# Save comprehensive report\n",
    "report_file = f\"{DIRS['results_phase2']}/PHASE2_COMPREHENSIVE_REPORT.json\"\n",
    "with open(report_file, 'w') as f:\n",
    "    json.dump(phase2_report, f, indent=4)\n",
    "\n",
    "print(f\"\\n✓ Comprehensive report saved: {report_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# PHASE 2 EXECUTION SUMMARY\")\n",
    "print(\"#\"*80)\n",
    "print(f\"\\nTotal Datasets: {len(datasets)}\")\n",
    "print(f\"Total Models Trained: {phase2_report['total_models']}\")\n",
    "print(f\"\\nOverall Best Model: {overall_best}\")\n",
    "print(f\"  • Average Accuracy: {phase2_report['overall_best_model']['avg_accuracy']:.4f}\")\n",
    "print(f\"  • Average F1-Score: {phase2_report['overall_best_model']['avg_f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Best Model per Dataset:\")\n",
    "for dataset, summary in phase2_report[\"performance_summary\"].items():\n",
    "    print(f\"\\n  {dataset}:\")\n",
    "    print(f\"    • Best Model: {summary['best_model']}\")\n",
    "    print(f\"    • F1-Score: {summary['best_f1_score']:.4f}\")\n",
    "    print(f\"    • Accuracy: {summary['best_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# PHASE 2 COMPLETED SUCCESSFULLY\")\n",
    "print(\"#\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "NEXT STEPS:\n",
    "1. Review model performance metrics and visualizations\n",
    "2. Proceed to Phase III: Advanced Deep Learning Models\n",
    "   - Notebook: 03_Advanced_DL_Models.ipynb\n",
    "   - Models: CNN, LSTM, Transformer\n",
    "3. Integrate Explainable AI (SHAP/LIME)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
